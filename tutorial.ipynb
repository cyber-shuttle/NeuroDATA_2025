{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdda4a23-9cc3-464c-827e-473b07af3daf",
   "metadata": {},
   "source": [
    "# Computing with Oscillators: A Speech Demo\n",
    "\n",
    "* author: Nand Chandravadia\n",
    "\n",
    "***\n",
    "\n",
    "This tutorial notebook is organized around three main concepts:\n",
    "\n",
    "- **Part 1: Speech Data**\n",
    "- **Part 2: Oscillator Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc1168-4b44-4802-b8e2-e09a2f8fdd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --force-reinstall airavata-python-sdk[notebook]\n",
    "\n",
    "import airavata_jupyter_magic\n",
    "\n",
    "\n",
    "%authenticate\n",
    "\n",
    "%request_runtime hpc_cpu --file=cybershuttle.yml --walltime=120 --use=NeuroData25VC1:cloud,expanse:shared,anvil:shared\n",
    "\n",
    "%switch_runtime hpc_cpu\n",
    "\n",
    "!git clone https://github.com/cyber-shuttle/NeuroDATA_2025 workspace\n",
    "%cd workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070dd945-3ace-46ac-99df-ae42f4ea43fd",
   "metadata": {},
   "source": [
    "## Part 1: Speech Data\n",
    "\n",
    "Speech is typically recorded on a microphone, such as the one on your cell-phone or laptop. Standard audio recorders usually record at sampling rates of 48 kHz. Here, we will look at two speech datasets of English and Arabic Speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96609da4-388d-4864-a8de-da514cecaca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Audio, Image, display\n",
    "from torch.distributions.uniform import Uniform\n",
    "\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Running on Device: {}\".format(device))\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b0cd2-c989-40ad-941c-3bcab64c53ff",
   "metadata": {},
   "source": [
    "### Load Speech Data\n",
    "\n",
    "All the speech data is found in `/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162817f-3e83-4aac-9497-a0e7032b3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"./data/7_06_47.wav\"\n",
    "audio_path = \"./data/0_60_25.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276d6d7-f9dc-4737-b37f-b78d73873a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_speech(path):\n",
    "\n",
    "    #load raw data\n",
    "    audio, sample_rate = torchaudio.load(path)\n",
    "\n",
    "    target_length = 48000\n",
    "    current_length = audio.shape[1]\n",
    "\n",
    "    padding = torch.zeros((audio.shape[0], target_length - current_length))\n",
    "    audio = torch.cat((audio, padding), dim=1)\n",
    "    \n",
    "\n",
    "    #downsample to 8kHz\n",
    "    original_sampling_rate = 48000\n",
    "    new_sampling_rate = 8000\n",
    "    \n",
    "    transform = torchaudio.transforms.Resample(orig_freq=original_sampling_rate, \n",
    "                                            new_freq=new_sampling_rate)\n",
    "\n",
    "    #appy new sampling rate\n",
    "    signal = transform(audio)\n",
    "    \n",
    "    #apply a normalization [-1, 1]\n",
    "    max_value = signal[0,:].abs().max()\n",
    "    new_signal=signal[0,:]*(1/max_value)\n",
    "    \n",
    "\n",
    "    return new_signal, new_sampling_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b1fa0-bfa5-4d0e-8997-79af8882db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_signal, new_sampling_rate = load_speech(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f5076-d94e-4ba0-ae1a-ad1ea68733a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=new_signal, rate=new_sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b4be2-f694-4440-b03a-0223c068a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.linspace(start = 0, stop = len(new_signal), num = len(new_signal))\n",
    "\n",
    "fig, axes = plt.subplots(1,1, figsize = (18, 5))\n",
    "\n",
    "axes.plot(time, new_signal, color = 'black')\n",
    "\n",
    "\n",
    "axes.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6086b063-0990-430b-af19-0fdd483a5343",
   "metadata": {},
   "source": [
    "# Part 2: Model: Network of Coupled Oscillators\n",
    "\n",
    "Now, let's look at the form and structure of the model. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"assets/network_oscillators.png\" alt=\"Oscillatory Network\" width=\"500\" height=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307088f3-e9e9-4d56-81bc-b9683c2e49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import coRNN\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc26a4b8-7022-4dcc-b724-15d76d8b669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Specify Model\n",
    "language = \"English\" #{\"English, Arabic\"}\n",
    "model_number = 1 #{1,2}\n",
    "isTrained = True\n",
    "\n",
    "######################################\n",
    "model_id = language + \"_\" + str(model_number)\n",
    "network_path_trained = \"./models/\" + language + \"_\" + str(model_number) + \"_\" + \"trained\" + \".pth\"\n",
    "network_path_untrained = \"./models/\" + language + \"_\" + str(model_number) + \"_\" + \"untrained\" + \".pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83190a-5e63-45b1-b158-9c0afec02b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyperparameters(model_id, device):\n",
    "    # Load the hyperparams file\n",
    "    with open(\"hyperparams.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # Choose which model config to use\n",
    "    model_config = config[\"models\"][model_id]\n",
    "\n",
    "    # Set the seed\n",
    "    generator = torch.Generator(device=device)\n",
    "    generator.manual_seed(model_config[\"random_seed\"])\n",
    "\n",
    "    # Define frequency range\n",
    "    low_frequency, high_frequency = 0.1, 20\n",
    "    gamma_tensor = (high_frequency - low_frequency) * torch.rand((1, model_config[\"n_hid\"]), generator=generator, device=device) + low_frequency\n",
    "\n",
    "    # Define damping range\n",
    "    low_damping, high_damping = 0.1, 80\n",
    "    epsilon_tensor = (high_damping - low_damping) * torch.rand((1, model_config[\"n_hid\"]), generator=generator, device=device) + low_damping\n",
    "\n",
    "    \n",
    "    # specific hyperparameters    \n",
    "    params = {\n",
    "        \"network_type\": model_config[\"network_type\"],\n",
    "        \"n_inp\": model_config[\"n_inp\"],\n",
    "        \"n_hid\": model_config[\"n_hid\"],\n",
    "        \"n_out\": model_config[\"n_out\"],\n",
    "        \"dt\": model_config[\"dt\"],\n",
    "        \"learning_rate\": model_config[\"learning_rate\"],\n",
    "        \"random_seed\": model_config[\"random_seed\"]\n",
    "    }\n",
    "\n",
    "    return params, gamma_tensor, epsilon_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf814e62-d466-4c24-a587-a0a474dd3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "params, gamma_tensor, epsilon_tensor = load_hyperparameters(model_id, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1bc40e-bb11-4e23-a055-10495ae773cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33635ef8-06cc-49c9-9ed8-06f6426634e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gamma_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbee50-26ce-4991-aa4d-1da773da3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epsilon_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25cbcfd-dfad-423b-bfcd-637129e46e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the untrained and trained model\n",
    "\n",
    "def load_model(network_path, params, gamma_tensor, epsilon_tensor, device):\n",
    "\n",
    "    #params\n",
    "    network_type = params[\"network_type\"]\n",
    "    n_inp = params[\"n_inp\"]\n",
    "    n_hid = params[\"n_hid\"]\n",
    "    n_out = params[\"n_out\"]\n",
    "    dt = params[\"dt\"]\n",
    "\n",
    "    #load model\n",
    "    model = coRNN(network_type = network_type, n_inp = n_inp, n_hid = n_hid,\n",
    "                                  n_out = n_out, dt = dt, \n",
    "                                  gamma = gamma_tensor, epsilon = epsilon_tensor)\n",
    "\n",
    "    # Load the saved state dictionary into the model\n",
    "    model.load_state_dict(torch.load(network_path, map_location=device))\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73eda53-8cee-42cf-956d-95a4b7f5e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_model = load_model(network_path=network_path_untrained, params=params, gamma_tensor=gamma_tensor, epsilon_tensor=epsilon_tensor, device=device)\n",
    "trained_model = load_model(network_path=network_path_trained, params=params, gamma_tensor=gamma_tensor, epsilon_tensor=epsilon_tensor, device=device)\n",
    "\n",
    "untrained_model_weights = untrained_model.state_dict()\n",
    "trained_model_weights = trained_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8cff02-c597-4df8-91cd-b657db7107e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD MODEL WEIGHTS\n",
    "\n",
    "def load_model_weights(model):\n",
    "\n",
    "    #load model weights!\n",
    "    recurrent_weights = model[\"cell.R.weight\"]\n",
    "    recurrent_velocity_weights = model[\"cell.F.weight\"]\n",
    "\n",
    "    return recurrent_weights, recurrent_velocity_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e1f3d-4688-419e-aa0a-2abe0c71eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model WEIGHTS\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "untrained_weights, untrained_damping = load_model_weights(untrained_model_weights)\n",
    "trained_weights, trained_damping = load_model_weights(trained_model_weights)\n",
    "\n",
    "index_start, index_end = 0, 64\n",
    "# Plot the first heatmap on the left subplot\n",
    "sns.heatmap(untrained_weights[index_start:index_end, index_start:index_end], ax=axes[0,0], cmap='coolwarm', linewidth=0.5)\n",
    "axes[0,0].set_title('Untrained Weight Matrix')\n",
    "\n",
    "sns.heatmap(untrained_damping[index_start:index_end, index_start:index_end], ax=axes[1,0], cmap='coolwarm', linewidth=0.5)\n",
    "axes[1,0].set_title('Untrained Damping Matrix')\n",
    "\n",
    "\n",
    "# Plot the second heatmap on the right subplot\n",
    "sns.heatmap(trained_weights[index_start:index_end, index_start:index_end], ax=axes[0,1],cmap='coolwarm', linewidth=0.5)\n",
    "axes[0,1].set_title('Trained Weight Matrix')\n",
    "\n",
    "sns.heatmap(trained_damping[index_start:index_end, index_start:index_end], ax=axes[1,1],cmap='coolwarm', linewidth=0.5)\n",
    "axes[1,1].set_title('Trained Damping Matrix')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927ab68-a1bf-41dc-9b8b-71c1bd15a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed the Model Speech!\n",
    "\n",
    "#format input\n",
    "new_signal = new_signal.reshape(1, 1, 8000) \n",
    "new_signal = new_signal.permute(2, 0, 1)\n",
    "input_signal = new_signal\n",
    "\n",
    "\n",
    "save_output, save_hy, save_hz, save_activation = trained_model(input_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e2efa1-f02f-4fcc-a009-f28770bc7140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What does the model think?\n",
    "\n",
    "def plot_response(signal, output, axes):\n",
    "\n",
    "    SAMPLING_FREQUENCY = 8000\n",
    "    TIME=1\n",
    "    \n",
    "    input = signal[:,0,0]\n",
    "    output = output[:, 0, :]\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    output = softmax(output)\n",
    "\n",
    "\n",
    "    color_map = {\n",
    "    0: \"navy\",\n",
    "    1: \"darkgreen\",\n",
    "    2: \"maroon\",\n",
    "    3: \"purple\",\n",
    "    4: \"teal\",\n",
    "    5: \"olive\",\n",
    "    6: \"sienna\",\n",
    "    7: \"royalblue\",\n",
    "    8: \"darkorange\",\n",
    "    9: \"indigo\"}\n",
    "\n",
    " \n",
    "\n",
    "    time = torch.arange(start=0, end=TIME, step=1/SAMPLING_FREQUENCY)\n",
    "\n",
    "    for target in range(0,10):\n",
    "        axes.plot(time, output.detach()[:, target], label = target, color = color_map[target], alpha = 0.7, linewidth=3)\n",
    "\n",
    "\n",
    "    #set axes\n",
    "    title = \"Model Prediction\"\n",
    "    axes.set_title(title, fontsize=24)\n",
    "    \n",
    "    axes.set_xlabel(\"Time (in seconds)\", fontsize=18)\n",
    "    axes.set_ylabel(\"Probability\", fontsize=18)\n",
    "    axes.set_ylim(-0.05,1.05)\n",
    "    axes.tick_params(axis='both', labelsize=18)  # Set tick label size \n",
    "    axes.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=15)\n",
    "    axes.grid(True)\n",
    "\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee985f-78aa-44ec-8015-98c5864439e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(18,5))\n",
    "\n",
    "plot_response(signal=input_signal, \n",
    "            output=save_output, \n",
    "            axes = axes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
